---
title: "Bias Evaluation of Toxicity Detection"
description: "Audited Detoxify for identity-based bias using Civil Comments and RealToxicityPrompts; proposed group-sensitive mitigations."
tags: ['Responsible AI', 'Fairness', 'Audit', 'Content Moderation', 'Ethics']
layout: default
author_profile: true
---


## ðŸ§© Problem

Automated content moderation tools can flag identity-related language disproportionately, impacting marginalized groups. This audit evaluated Detoxifyâ€™s performance across identity subgroups.

## ðŸ”§ Pipeline Overview

- **Data**: Civil Comments and RealToxicityPrompts.
- **Preprocessing**: Cleaned and standardized toxic/non-toxic labels.
- **Analysis**: Grouped by identity (e.g., Muslim, gay, Black); evaluated FPR, FNR parity.
- **Modeling**: Black-box inference of Detoxify; error clustering by group.
- **Evaluation**: Found up to 20% recall gap for minority groups and consistent overflagging.

## ðŸš€ Impact

The audit revealed risks masked by global metrics. Mitigation recommendations included group-specific thresholds and counterfactual data augmentation. Findings highlighted fairness risks in real-world deployments.
        