---
title: "When Global Accuracy Lies: Why We Need Identity-Aware Fairness Metrics"
excerpt: "A 20% recall gap for marginalized groups, hidden behind a 92% overall score? This is why we audit."
layout: single
author_profile: true
---


Content moderation models can seem accurateâ€”until you ask how they treat sentences about LGBTQ+ or Muslim communities. I used Civil Comments and RealToxicityPrompts to test Detoxify, a widely used toxicity classifier.

Even though the model hit over 90% accuracy, it misclassified *non-toxic* identity-related sentences at disproportionate rates. Words like â€œgay,â€ â€œMuslim,â€ or â€œBlackâ€ triggered false positives due to biased training data.

ğŸ“Š **Insight:** I measured recall and false positive rate by identity group. Gaps of 15â€“20% were commonâ€”meaning the model disproportionately silenced certain groups.

ğŸš¨ **So what?** Fairness isnâ€™t about perfection. Itâ€™s about visibility. Unless we measure harm at the subgroup level, weâ€™re flying blind.

Structured audits like this arenâ€™t just ethicalâ€”theyâ€™re *product critical*. If we care about scale, we have to care about whoâ€™s affected.
        